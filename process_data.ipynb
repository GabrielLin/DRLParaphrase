{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import hashlib\n",
    "import struct\n",
    "import subprocess\n",
    "import collections\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from tensorflow.core.example import example_pb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.tokenize import word_tokenize\n",
    "from nltk.parse import CoreNLPParser\n",
    "parser = CoreNLPParser(url='http://localhost:8889')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preliminary data filtering/analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(fpath):\n",
    "    # Move to dataframe for text usage\n",
    "    df = pd.read_csv(fpath)\n",
    "    print(\"Total number of pairs:\", df.shape[0])\n",
    "    \n",
    "    # Convert all questions to string and X, y format for splitting\n",
    "    train_q1 = [str(el) for el in train_df[\"question1\"]]\n",
    "    train_q2 = [str(el) for el in train_df[\"question2\"]]\n",
    "    X = [(q1, q2) for q1, q2 in zip(train_q1, train_q2)]\n",
    "    y = list(train_df[\"is_duplicate\"])\n",
    "    \n",
    "    # Sample splitting 60% train, 20% validation and 20% test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1)\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of pairs: 404290\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_val, y_val, X_test, y_test = process_data(\"/Users/dfirebanks/Projects/DRLParaphrase/quora/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read csv\n",
    "train_df = pd.read_csv(\"/Users/dfirebanks/Projects/DRLParaphrase/quora/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404290, 6)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>Astrology: I am a Capricorn Sun Cap moon and c...</td>\n",
       "      <td>I'm a triple Capricorn (Sun, Moon and ascendan...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>Should I buy tiago?</td>\n",
       "      <td>What keeps childern active and far from phone ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>How can I be a good geologist?</td>\n",
       "      <td>What should I do to be a great geologist?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "      <td>When do you use シ instead of し?</td>\n",
       "      <td>When do you use \"&amp;\" instead of \"and\"?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>Motorola (company): Can I hack my Charter Moto...</td>\n",
       "      <td>How do I hack Motorola DCX3400 for free internet?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>21</td>\n",
       "      <td>22</td>\n",
       "      <td>Method to find separation of slits using fresn...</td>\n",
       "      <td>What are some of the things technicians can te...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>23</td>\n",
       "      <td>24</td>\n",
       "      <td>How do I read and find my YouTube comments?</td>\n",
       "      <td>How can I see all my Youtube comments?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>25</td>\n",
       "      <td>26</td>\n",
       "      <td>What can make Physics easy to learn?</td>\n",
       "      <td>How can you make physics easy to learn?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>27</td>\n",
       "      <td>28</td>\n",
       "      <td>What was your first sexual experience like?</td>\n",
       "      <td>What was your first sexual experience?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>29</td>\n",
       "      <td>30</td>\n",
       "      <td>What are the laws to change your status from a...</td>\n",
       "      <td>What are the laws to change your status from a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>31</td>\n",
       "      <td>32</td>\n",
       "      <td>What would a Trump presidency mean for current...</td>\n",
       "      <td>How will a Trump presidency affect the student...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>33</td>\n",
       "      <td>34</td>\n",
       "      <td>What does manipulation mean?</td>\n",
       "      <td>What does manipulation means?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>35</td>\n",
       "      <td>36</td>\n",
       "      <td>Why do girls want to be friends with the guy t...</td>\n",
       "      <td>How do guys feel after rejecting a girl?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>37</td>\n",
       "      <td>38</td>\n",
       "      <td>Why are so many Quora users posting questions ...</td>\n",
       "      <td>Why do people ask Quora questions which can be...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>39</td>\n",
       "      <td>40</td>\n",
       "      <td>Which is the best digital marketing institutio...</td>\n",
       "      <td>Which is the best digital marketing institute ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  qid1  qid2                                          question1  \\\n",
       "0    0     1     2  What is the step by step guide to invest in sh...   \n",
       "1    1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2    2     5     6  How can I increase the speed of my internet co...   \n",
       "3    3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4    4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "5    5    11    12  Astrology: I am a Capricorn Sun Cap moon and c...   \n",
       "6    6    13    14                                Should I buy tiago?   \n",
       "7    7    15    16                     How can I be a good geologist?   \n",
       "8    8    17    18                    When do you use シ instead of し?   \n",
       "9    9    19    20  Motorola (company): Can I hack my Charter Moto...   \n",
       "10  10    21    22  Method to find separation of slits using fresn...   \n",
       "11  11    23    24        How do I read and find my YouTube comments?   \n",
       "12  12    25    26               What can make Physics easy to learn?   \n",
       "13  13    27    28        What was your first sexual experience like?   \n",
       "14  14    29    30  What are the laws to change your status from a...   \n",
       "15  15    31    32  What would a Trump presidency mean for current...   \n",
       "16  16    33    34                       What does manipulation mean?   \n",
       "17  17    35    36  Why do girls want to be friends with the guy t...   \n",
       "18  18    37    38  Why are so many Quora users posting questions ...   \n",
       "19  19    39    40  Which is the best digital marketing institutio...   \n",
       "\n",
       "                                            question2  is_duplicate  \n",
       "0   What is the step by step guide to invest in sh...             0  \n",
       "1   What would happen if the Indian government sto...             0  \n",
       "2   How can Internet speed be increased by hacking...             0  \n",
       "3   Find the remainder when [math]23^{24}[/math] i...             0  \n",
       "4             Which fish would survive in salt water?             0  \n",
       "5   I'm a triple Capricorn (Sun, Moon and ascendan...             1  \n",
       "6   What keeps childern active and far from phone ...             0  \n",
       "7           What should I do to be a great geologist?             1  \n",
       "8               When do you use \"&\" instead of \"and\"?             0  \n",
       "9   How do I hack Motorola DCX3400 for free internet?             0  \n",
       "10  What are some of the things technicians can te...             0  \n",
       "11             How can I see all my Youtube comments?             1  \n",
       "12            How can you make physics easy to learn?             1  \n",
       "13             What was your first sexual experience?             1  \n",
       "14  What are the laws to change your status from a...             0  \n",
       "15  How will a Trump presidency affect the student...             1  \n",
       "16                      What does manipulation means?             1  \n",
       "17           How do guys feel after rejecting a girl?             0  \n",
       "18  Why do people ask Quora questions which can be...             1  \n",
       "19  Which is the best digital marketing institute ...             0  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all questions to string and X, y format for splitting\n",
    "train_q1 = [str(el) for el in train_df[\"question1\"]]\n",
    "train_q2 = [str(el) for el in train_df[\"question2\"]]\n",
    "X = [(q1, q2) for q1, q2 in zip(train_q1, train_q2)]\n",
    "y = list(train_df[\"is_duplicate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_word_count = [len(str(q).split(\" \")) for q in train_df[\"question1\"]]\n",
    "q2_word_count = [len(str(q).split(\" \")) for q in train_df[\"question2\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample splitting 60% train, 20% validation and 20% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of questions with more than 20 words for each column:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6.365727571792525, 7.723169012342625)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For initial processing purposes, we calculate the number of questions with more than 20 words \n",
    "count1 = 0\n",
    "count2 = 0\n",
    "\n",
    "for l1, l2 in zip(q1_word_count, q2_word_count):\n",
    "    if l1 > 20:\n",
    "        count1 += 1\n",
    "    if l2 > 20:\n",
    "        count2 += 1\n",
    "\n",
    "print(\"Percentage of questions with more than 20 words for each column:\")\n",
    "100*count1/len(q1_word_count), 100*count2/len(q2_word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". 3306\n",
      "? 13016\n",
      "? 20794\n",
      "HH 23884\n",
      "Na 44619\n",
      "I'm  54029\n",
      "? 96725\n",
      "ok ? 102512\n",
      "? 104101\n",
      "i 108978\n",
      "What 109311\n",
      "o 115347\n",
      "? 134403\n",
      "o 151922\n",
      "A 158778\n",
      "Nana 164553\n",
      "I'm  175584\n",
      "spam 180461\n",
      "? 189659\n",
      "‘ 190570\n",
      "I 199110\n",
      "? 208485\n",
      "Can? 208798\n",
      "? 213220\n",
      "Aaas 216861\n",
      "? 254161\n",
      "hi 257077\n",
      "Why? 260779\n",
      "delete 263134\n",
      "Ok 270146\n",
      "? 273065\n",
      "Why? 324777\n",
      "My 325530\n",
      "My 328601\n",
      "Who is 329933\n",
      "Q? 351788\n",
      "H 357127\n",
      "no 381124\n",
      "? 402423\n"
     ]
    }
   ],
   "source": [
    "# Samples of nonsensical questions\n",
    "for i, q in enumerate(train_q1):\n",
    "    if len(q) == 1:\n",
    "        print(q, i)\n",
    "    if len(q) == 2:\n",
    "        print(q, i)\n",
    "    if len(q) == 4:\n",
    "        print(q, i)\n",
    "    if len(q) == 6:\n",
    "        print(q, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 5000\n",
    "\n",
    "train_tokenized_dir = \"/Users/dfirebanks/Projects/DRLParaphrase/quora/train_tokens\"\n",
    "val_tokenized_dir = \"/Users/dfirebanks/Projects/DRLParaphrase/quora/val_tokens\"\n",
    "test_tokenized_dir = \"/Users/dfirebanks/Projects/DRLParaphrase/quora/test_tokens\"\n",
    "finished_files_dir = \"/Users/dfirebanks/Projects/DRLParaphrase/quora/finished_files/\"\n",
    "\n",
    "CHUNK_SIZE = 1000 # num examples per chunk, for the chunked data\n",
    "chunks_dir = os.path.join(finished_files_dir, \"chunked\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_questions(questions):\n",
    "    \"\"\"Takes in a list of question pairs, returns a list of tokenized question pairs\"\"\"\n",
    "    tokenized_qs = []\n",
    "    for i in range(len(questions)):\n",
    "        # Tokenizing both question1 and question2\n",
    "        tokenized_qs.append([list(parser.tokenize(questions[i][0])), list(parser.tokenize(questions[i][1]))])\n",
    "        if i % 10000 == 0:\n",
    "            print(f\"Tokenized {i} questions!\")\n",
    "            print(f\"Q1: {questions[i][0]} \\nQ2: {questions[i][1]}\")\n",
    "    return tokenized_qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_tokens(questions, outdir):\n",
    "    \"\"\" Stores a pair of tokenized questions separated by a new line\"\"\"\n",
    "    \n",
    "    fnum = 0\n",
    "    for qs in questions: \n",
    "        with open(os.path.join(outdir, \"qpair\" + str(fnum) + \".tokens\"), \"w\") as f:\n",
    "            e1 = ' '.join(qs[0]).lower().strip()\n",
    "            e2 = ' '.join(qs[1]).lower().strip()\n",
    "            f.write(f\"Q1: {e1} \\nQ2: {e2}\\n\")\n",
    "        \n",
    "        fnum += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized 0 questions!\n",
      "Q1: How can I become a problem solver? \n",
      "Q2: How do I become a better thinker, innovator and a problem solver?\n",
      "Tokenized 10000 questions!\n",
      "Q1: Can I crack the JEE Mains without coaching in two months? \n",
      "Q2: Can I crack the JEE Mains in two months without any coaching?\n",
      "Tokenized 20000 questions!\n",
      "Q1: What is your opinion on PM Narendra Modi's decision to ban INR 500 and INR 1000 notes? \n",
      "Q2: What is your reaction about the ban on Rs. 500 and Rs. 1000 notes? Won't it create a chaos and harm the economy?\n",
      "Tokenized 30000 questions!\n",
      "Q1: How does the new change of elimination of master’s degree exemption impact the H-1B holders and H-1B applicants? \n",
      "Q2: Can H-1B visa holders do higher studies in the US?\n",
      "Tokenized 40000 questions!\n",
      "Q1: Why isn't Hillary Clinton in jail? \n",
      "Q2: Why could Hillary Clinton go to jail?\n",
      "Tokenized 50000 questions!\n",
      "Q1: What are the differences between a Fibonacci heap and a binomial heap? \n",
      "Q2: What is the difference between a Fibonacci heap and Binomial heap?\n",
      "Tokenized 60000 questions!\n",
      "Q1: Can you access adult channels on Roku? \n",
      "Q2: How do you add Adult Channels on Roku?\n",
      "Tokenized 70000 questions!\n",
      "Q1: How do you give a fuck? \n",
      "Q2: How can I hack WiFi using a command prompt?\n",
      "Tokenized 80000 questions!\n",
      "Q1: How can I pass the GRE exam in physics? \n",
      "Q2: How can pass the GRE exam in physics?\n",
      "Tokenized 90000 questions!\n",
      "Q1: How do you cope with the loss of a pet you have had since you were a baby? \n",
      "Q2: How do I cope with the loss of a pet?\n",
      "Tokenized 100000 questions!\n",
      "Q1: How can we improve India's current education system? \n",
      "Q2: How can we change India's education system?\n",
      "Tokenized 110000 questions!\n",
      "Q1: Why do codons consist of three nucleotides? \n",
      "Q2: What do nucleotides contain?\n",
      "Tokenized 120000 questions!\n",
      "Q1: Why shouldn't I study cognitive science? \n",
      "Q2: What are jobs relating to cognitive science?\n",
      "Tokenized 130000 questions!\n",
      "Q1: If you knew you were going to die tomorrow, what would you like to finish today? \n",
      "Q2: ’what would you do if you knew you would die tomorrow?\n",
      "Tokenized 140000 questions!\n",
      "Q1: Where do we apply simultaneous equations? \n",
      "Q2: For what do we use simultaneous equations for?\n",
      "Tokenized 150000 questions!\n",
      "Q1: What is the difference between MCB and MCCB? Where can it be used? \n",
      "Q2: What is the resultant EMF of two cells in a parallel combination?\n",
      "Tokenized 160000 questions!\n",
      "Q1: What is the size comparison between Death Star I, Death Star II and the Starkiller Base? \n",
      "Q2: Was the Starkiller created before the first Death Star?\n",
      "Tokenized 170000 questions!\n",
      "Q1: Is Turkish-Greek confederation is realistic in the future? \n",
      "Q2: Is it possible to have a Greek-Turkish Confederation in the future?\n",
      "Tokenized 180000 questions!\n",
      "Q1: What are the most interesting products and innovations that Berkshire Bank is coming out with in 2016? \n",
      "Q2: What are the most interesting products and innovations that Green Bank is coming out with in 2016?\n",
      "Tokenized 190000 questions!\n",
      "Q1: What will happen to Hillary Clinton after the 2016 election? \n",
      "Q2: What will Hillary Clinton do after the election?\n",
      "Tokenized 200000 questions!\n",
      "Q1: Why does Krishna address Arjun as Parth? \n",
      "Q2: Why is Krishna closest to Arjun in Mahabharata?\n",
      "Tokenized 210000 questions!\n",
      "Q1: What is the point and meaning of the episode \"Fly\" in Breaking Bad? \n",
      "Q2: Breaking Bad Season 5: Episode 10 (\"Buried\"): Shouldn't Hank have some good, hard evidence at this point?\n",
      "Tokenized 220000 questions!\n",
      "Q1: I have a shih tzu puppy. What is a good brand of puppy food? \n",
      "Q2: What is the best puppy food?\n",
      "Tokenized 230000 questions!\n",
      "Q1: Can I get some project ideas for my final year computer engineering project? Can some of you suggest me? \n",
      "Q2: How do I get some of the best ideas for a final year project?\n",
      "Tokenized 240000 questions!\n",
      "Q1: What is Cross-platform Mobile App Development? \n",
      "Q2: What is cross platform mobile apps?\n",
      "Tokenized 0 questions!\n",
      "Q1: What are the top challenges cybersecurity faces today? \n",
      "Q2: What are the top cybersecurity challenges?\n",
      "Tokenized 10000 questions!\n",
      "Q1: I am repairing a soft steel chair by arc welding and keep burning holes in the metal, what can I do to make a strong joint? \n",
      "Q2: Since Carrier has been granted a tax break, can other companies threaten to ship jobs elsewhere if they do not get a tax break as well?\n",
      "Tokenized 20000 questions!\n",
      "Q1: Do famous companies like McDonald's, Chick Fil A etc handle their customer reviews or complaints via their own customer service reps or outsource it? \n",
      "Q2: How can I reach Uber customer service? Do they have a phone number?\n",
      "Tokenized 30000 questions!\n",
      "Q1: What are the top universities in Germany for an MS in civil engineering? \n",
      "Q2: Which are the best universities in Germany for civil engneering for MS?\n",
      "Tokenized 40000 questions!\n",
      "Q1: Are girls allowed in the merchant navy in India, or not? \n",
      "Q2: How do I get my parents to believe that Merchant Navy is good for girls?\n",
      "Tokenized 50000 questions!\n",
      "Q1: What must I do to have good handwriting? \n",
      "Q2: What can I do improve my handwriting?\n",
      "Tokenized 60000 questions!\n",
      "Q1: Australian Economic Law? \n",
      "Q2: What is full form of windows?\n",
      "Tokenized 70000 questions!\n",
      "Q1: Should India declare a war against Pakistan now? \n",
      "Q2: Why doesn’t PM Narendra Modi attack Pakistan, even after the recent Uri attack in India?\n",
      "Tokenized 80000 questions!\n",
      "Q1: What are the best online resources? \n",
      "Q2: What are the best online resources on biology?\n",
      "Tokenized 0 questions!\n",
      "Q1: How can I stop playing video games? \n",
      "Q2: Should I stop playing video games with my child?\n",
      "Tokenized 10000 questions!\n",
      "Q1: What are some dumb questions ever asked on Quora? \n",
      "Q2: What are the most annoying questions that you come across in Quora?\n",
      "Tokenized 20000 questions!\n",
      "Q1: How do I find a job for post of an Odoo developer as a MCA Fresher in Ahmedabad? \n",
      "Q2: Does God send animals to hell?\n",
      "Tokenized 30000 questions!\n",
      "Q1: But what is meditation? \n",
      "Q2: How do you start a meditation habit?\n",
      "Tokenized 40000 questions!\n",
      "Q1: Which is a better organization in terms of HR, work days and technology, Accenture or Deloitte? \n",
      "Q2: Which company is better to work with as a computer engineer fresher, TCS or Accenture?\n",
      "Tokenized 50000 questions!\n",
      "Q1: How can I lose post marriage weight? \n",
      "Q2: How can I efficiently lose weight?\n",
      "Tokenized 60000 questions!\n",
      "Q1: What is the scariest paranormal experience you've ever had? \n",
      "Q2: Have you ever had any paranormal experience?\n",
      "Tokenized 70000 questions!\n",
      "Q1: What are some great references to learn web design? \n",
      "Q2: Where can I learn web design?\n",
      "Tokenized 80000 questions!\n",
      "Q1: What should I do to make my Lego Mindstorms not wobble while crossing the black crack between 2 tiles? \n",
      "Q2: How competitive is the hiring process at Laredo Petroleum?\n"
     ]
    }
   ],
   "source": [
    "# Create tokenized questions\n",
    "train_tokens = tokenize_questions(X_train)\n",
    "val_tokens = tokenize_questions(X_val)\n",
    "test_tokens = tokenize_questions(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store tokenized questions\n",
    "train_tokenized_dir = \"/Users/dfirebanks/Projects/DRLParaphrase/quora/train_tokens1\"\n",
    "val_tokenized_dir = \"/Users/dfirebanks/Projects/DRLParaphrase/quora/val_tokens1\"\n",
    "test_tokenized_dir = \"/Users/dfirebanks/Projects/DRLParaphrase/quora/test_tokens1\"\n",
    "\n",
    "store_tokens(train_tokens, train_tokenized_dir)\n",
    "store_tokens(val_tokens, val_tokenized_dir)\n",
    "store_tokens(test_tokens, test_tokenized_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Storing files for model (.bin, tf.Example())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_bin(tokenized_qs, is_duplicate, outfile, makevocab=False):\n",
    "    \"\"\" Creates bin files given pairs of tokenized questions, an outfile name and if applicable, creates a vocabulary file \n",
    "        \n",
    "        @tokenized_qs: list of questions as tokenized strings [(question1_tokenized, question2_tokenized), (tokenized_pair_2), ...]\n",
    "        @is_duplicate: label list, target variable\n",
    "        @outfile: path as string\"\"\"\n",
    "    \n",
    "    if makevocab:\n",
    "        vocab_counter = collections.Counter()\n",
    "        \n",
    "    with open(outfile, 'wb') as writer:\n",
    "        \n",
    "        for i in range(len(tokenized_qs)):\n",
    "            tok_q1 = tokenized_qs[i][0]\n",
    "            tok_q2 = tokenized_qs[i][1]\n",
    "            target = str(is_duplicate[i])\n",
    "            # TODO Important note: Max length of question is 20 words, I assume we clean up the symbols that are not question marks? OR we do this in the actual program\n",
    "            # In the original program, there was no particular data cleaning, so I assume that this is done afterwards\n",
    "            \n",
    "            # Questions as strings: lowercase and strip them \n",
    "            q1 = ' '.join(tok_q1).lower().strip()\n",
    "            q2 = ' '.join(tok_q2).lower().strip()\n",
    "            \n",
    "            # Write to tf.Example\n",
    "            tf_example = example_pb2.Example()\n",
    "            tf_example.features.feature['question1'].bytes_list.value.extend([q1.encode()])\n",
    "            tf_example.features.feature['question2'].bytes_list.value.extend([q2.encode()])\n",
    "            tf_example.features.feature['target'].bytes_list.value.extend([target.encode()])\n",
    "            \n",
    "            tf_example_str = tf_example.SerializeToString()\n",
    "            str_len = len(tf_example_str)\n",
    "            writer.write(struct.pack('q', str_len)) # write length of string\n",
    "            writer.write(struct.pack('%ds' % str_len, tf_example_str)) # write string of length noted earlier\n",
    "            \n",
    "            \n",
    "            # Make the vocab to write, if applicable\n",
    "            if makevocab:\n",
    "                tokens = q1 + q2\n",
    "                tokens = [t.strip() for t in tokens] # strip\n",
    "                tokens = [t for t in tokens if t != \"\"] # remove empty\n",
    "                vocab_counter.update(tokens)\n",
    "    \n",
    "    print(\"Finished writing file %s\\n\" % outfile)\n",
    "    \n",
    "    # Write vocab to file\n",
    "    if makevocab:\n",
    "        print(\"Writing vocab file...\")\n",
    "        with open(os.path.join(finished_files_dir, \"vocab\"), 'w') as writer:\n",
    "            for word, count in vocab_counter.most_common(VOCAB_SIZE):\n",
    "                writer.write(word + ' ' + str(count) + '\\n')\n",
    "        print(\"Finished writing vocab file\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished writing file /Users/dfirebanks/Projects/DRLParaphrase/quora/finished_files1/train.bin\n",
      "\n",
      "Writing vocab file...\n",
      "Finished writing vocab file\n",
      "Finished writing file /Users/dfirebanks/Projects/DRLParaphrase/quora/finished_files1/val.bin\n",
      "\n",
      "Finished writing file /Users/dfirebanks/Projects/DRLParaphrase/quora/finished_files1/test.bin\n",
      "\n"
     ]
    }
   ],
   "source": [
    "finished_files_dir = \"/Users/dfirebanks/Projects/DRLParaphrase/quora/finished_files1/\"\n",
    "# Store all .bin files\n",
    "write_to_bin(train_tokens, y_train, os.path.join(finished_files_dir, \"train.bin\"), makevocab=True)\n",
    "write_to_bin(val_tokens, y_val, os.path.join(finished_files_dir, \"val.bin\"))\n",
    "write_to_bin(test_tokens, y_test, os.path.join(finished_files_dir, \"test.bin\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Chunk files in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk functions for replications of paper, not sure if truly necessary?\n",
    "def chunk_file(set_name, files_dir):\n",
    "    in_file = files_dir % set_name\n",
    "    reader = open(in_file, \"rb\")\n",
    "    chunk = 0\n",
    "    finished = False\n",
    "    while not finished:\n",
    "        chunk_fname = os.path.join(chunks_dir, '%s_%03d.bin' % (set_name, chunk)) # new chunk\n",
    "        with open(chunk_fname, 'wb') as writer:\n",
    "            for _ in range(CHUNK_SIZE):\n",
    "                len_bytes = reader.read(8)\n",
    "                if not len_bytes:\n",
    "                    finished = True\n",
    "                    break\n",
    "                str_len = struct.unpack('q', len_bytes)[0]\n",
    "                example_str = struct.unpack('%ds' % str_len, reader.read(str_len))[0]\n",
    "                writer.write(struct.pack('q', str_len))\n",
    "                writer.write(struct.pack('%ds' % str_len, example_str))\n",
    "            chunk += 1\n",
    "\n",
    "\n",
    "def chunk_all(files_dir):\n",
    "    # Make a dir to hold the chunks\n",
    "    if not os.path.isdir(chunks_dir):\n",
    "        os.mkdir(chunks_dir)\n",
    "    # Chunk the data\n",
    "    for set_name in ['train', 'val', 'test']:\n",
    "        print(\"Splitting %s data into chunks...\" % set_name)\n",
    "        chunk_file(set_name, files_dir)\n",
    "    print(\"Saved chunked data in %s\" % chunks_dir)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting train data into chunks...\n",
      "Splitting val data into chunks...\n",
      "Splitting test data into chunks...\n",
      "Saved chunked data in /Users/dfirebanks/Projects/DRLParaphrase/quora/finished_files1/chunked\n"
     ]
    }
   ],
   "source": [
    "finished_files_dir = \"/Users/dfirebanks/Projects/DRLParaphrase/quora/finished_files1/\"\n",
    "chunks_dir = os.path.join(finished_files_dir, \"chunked\")\n",
    "chunk_all('/Users/dfirebanks/Projects/DRLParaphrase/quora/finished_files1/%s.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Combine the whole process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 5000\n",
    "\n",
    "train_tokenized_dir = \"/Users/dfirebanks/Projects/DRLParaphrase/quora/train_tokens\"\n",
    "val_tokenized_dir = \"/Users/dfirebanks/Projects/DRLParaphrase/quora/val_tokens\"\n",
    "test_tokenized_dir = \"/Users/dfirebanks/Projects/DRLParaphrase/quora/test_tokens\"\n",
    "finished_files_dir = \"/Users/dfirebanks/Projects/DRLParaphrase/quora/finished_files1/\"\n",
    "\n",
    "CHUNK_SIZE = 1000 # num examples per chunk, for the chunked data\n",
    "chunks_dir = os.path.join(finished_files_dir, \"chunked\")\n",
    "\n",
    "def main():\n",
    "    \"\"\" 1. Get the train and test set paths, parse them into texts\n",
    "        2. Tokenize all of them\n",
    "        3. Create bin files \n",
    "        4. Chunk data \n",
    "    \"\"\"\n",
    "\n",
    "    # Alternative use: input file path from command line\n",
    "    if len(sys.argv) != 1:\n",
    "        print(\"USAGE: python make_datafiles.py\")\n",
    "        sys.exit()\n",
    "        \n",
    "    \n",
    "    fpath = \"/Users/dfirebanks/Projects/DRLParaphrase/quora/train.csv\"\n",
    "\n",
    "    # Move to dataframe for text usage\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = process_data(fpath)\n",
    "\n",
    "    # Create some new directories to store tokenized versions of the questions\n",
    "    if not os.path.exists(train_tokenized_dir): \n",
    "        os.makedirs(train_tokenized_dir)\n",
    "    if not os.path.exists(val_tokenized_dir): \n",
    "        os.makedirs(val_tokenized_dir)\n",
    "    if not os.path.exists(test_tokenized_dir): \n",
    "        os.makedirs(test_tokenized_dir)\n",
    "    if not os.path.exists(finished_files_dir): \n",
    "        os.makedirs(finished_files_dir)\n",
    "\n",
    "    # Run stanford tokenizer on both sets, outputting to tokenized questions directories\n",
    "    train_tokens = tokenize_questions(X_train)\n",
    "    val_tokens = tokenize_questions(X_val)\n",
    "    test_tokens = tokenize_questions(X_test)\n",
    "    \n",
    "    store_tokens(train_tokens, train_tokenized_dir)\n",
    "    store_tokens(val_tokens, val_tokenized_dir)\n",
    "    store_tokens(test_tokens, test_tokenized_dir)\n",
    "    \n",
    "    # Read the tokenized stories, do a little postprocessing then write to bin files\n",
    "    write_to_bin(train_tokens, y_train, os.path.join(finished_files_dir, \"train.bin\"), makevocab=True)\n",
    "    write_to_bin(val_tokens, y_val, os.path.join(finished_files_dir, \"val.bin\"))\n",
    "    write_to_bin(test_tokens, y_test, os.path.join(finished_files_dir, \"test.bin\"))\n",
    "    \n",
    "    # Chunk the data. This splits each of train.bin, val.bin and test.bin into smaller chunks, each containing e.g. 1000 examples, and saves them in finished_files/chunks\n",
    "    chunk_all('/Users/dfirebanks/Projects/DRLParaphrase/quora/finished_files/%s.bin')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
